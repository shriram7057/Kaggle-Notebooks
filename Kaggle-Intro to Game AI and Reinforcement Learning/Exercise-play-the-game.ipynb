{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17592,"databundleVersionId":899221,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/play-the-game).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nYou have seen how to define a random agent.  In this exercise, you'll make a few improvements.\n\nTo get started, run the code cell below to set up our feedback system.","metadata":{}},{"cell_type":"code","source":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex1 import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:23:31.425789Z","iopub.execute_input":"2026-01-05T08:23:31.426074Z","iopub.status.idle":"2026-01-05T08:23:31.431771Z","shell.execute_reply.started":"2026-01-05T08:23:31.426054Z","shell.execute_reply":"2026-01-05T08:23:31.430692Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### 1) A smarter agent\n\nWe can improve the performance without devising a complicated strategy, simply by selecting a winning move, if one is available.\n\nIn this exercise, you will create an agent that:\n- selects the winning move, if it is available.  (_If there is more than one move that lets the agent win the game, the agent can select any of them._)\n- Otherwise, it should select a random move.\n\nTo help you with this goal, we provide some helper functions in the code cell below. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Gets board at next step if agent drops piece in selected column\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\n# Returns True if dropping piece in column results in game win\ndef check_winning_move(obs, config, col, piece):\n    # Convert the board to a 2D grid\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    next_grid = drop_piece(grid, col, piece, config)\n    # horizontal\n    for row in range(config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[row,col:col+config.inarow])\n            if window.count(piece) == config.inarow:\n                return True\n    # vertical\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns):\n            window = list(next_grid[row:row+config.inarow,col])\n            if window.count(piece) == config.inarow:\n                return True\n    # positive diagonal\n    for row in range(config.rows-(config.inarow-1)):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    # negative diagonal\n    for row in range(config.inarow-1, config.rows):\n        for col in range(config.columns-(config.inarow-1)):\n            window = list(next_grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n            if window.count(piece) == config.inarow:\n                return True\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:24:27.361875Z","iopub.execute_input":"2026-01-05T08:24:27.362160Z","iopub.status.idle":"2026-01-05T08:24:27.370522Z","shell.execute_reply.started":"2026-01-05T08:24:27.362138Z","shell.execute_reply":"2026-01-05T08:24:27.369727Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"The `check_winning_move()` function takes four required arguments: the first two (`obs` and `config`) should be familiar, and: \n- `col` is any valid move \n- `piece` is either the agent's mark or the mark of its opponent.  \n\nThe function returns `True` if dropping the piece in the provided column wins the game (for either the agent or its opponent), and otherwise returns `False`.  To check if the agent can win in the next move, you should set `piece=obs.mark`.\n\n**To complete this exercise, you need to define `agent_q1()` in the code cell below.  To do this, you're encouraged to use the `check_winning_move()` function.**  \n\nThe `drop_piece()` function (defined in the code cell above) is called in the `check_winning_move()` function.  Feel free to examine the details, but you won't need a detailed understanding to solve the exercise.","metadata":{}},{"cell_type":"code","source":"import random\n\ndef agent_q1(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    \n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    \n    return random.choice(valid_moves)\n\nq_1.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:24:57.464500Z","iopub.execute_input":"2026-01-05T08:24:57.464820Z","iopub.status.idle":"2026-01-05T08:24:57.495524Z","shell.execute_reply.started":"2026-01-05T08:24:57.464793Z","shell.execute_reply":"2026-01-05T08:24:57.494537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.2, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_SelectWinning\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2) An even smarter agent\n\nIn the previous question, you created an agent that selects winning moves.  In this problem, you'll amend the code to create an agent that can also block its opponent from winning.  In particular, your agent should:\n- Select a winning move, if one is available.\n- Otherwise, it selects a move to block the opponent from winning, if the opponent has a move that it can play in its next turn to win the game. \n- If neither the agent nor the opponent can win in their next moves, the agent selects a random move.\n\nTo help you with this exercise, you are encouraged to start with the agent from the previous exercise.  \n\n**To check if the opponent has a winning move, you can use the `check_winning_move()` function, but you'll need to supply a different value for the `piece` argument.**  ","metadata":{}},{"cell_type":"code","source":"import random\n\ndef agent_q2(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    \n    # Check for a winning move\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    \n    # Check for a blocking move\n    opponent_mark = 1 if obs.mark == 2 else 2\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, opponent_mark):\n            return col\n    \n    # Otherwise, choose a random move\n    return random.choice(valid_moves)\n\nq_2.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:25:53.528405Z","iopub.execute_input":"2026-01-05T08:25:53.528709Z","iopub.status.idle":"2026-01-05T08:25:53.573369Z","shell.execute_reply.started":"2026-01-05T08:25:53.528690Z","shell.execute_reply":"2026-01-05T08:25:53.572098Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.2, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_BlockOpponent\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3) Looking ahead\n\nSo far, you have encoded an agent that always selects the winning move, if it's available.  And, it can also block the opponent from winning.\n\nYou might expect that this agent should perform quite well!  But how is it still possible that it can still lose the game?","metadata":{}},{"cell_type":"code","source":"#q_3.hint()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q_3.solution()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:26:45.996094Z","iopub.execute_input":"2026-01-05T08:26:45.996359Z","iopub.status.idle":"2026-01-05T08:26:46.005732Z","shell.execute_reply.started":"2026-01-05T08:26:45.996341Z","shell.execute_reply":"2026-01-05T08:26:46.004854Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"3_WhyNotOptimal\", \"learnToolsVersion\": \"0.3.5\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: The agent can still lose the game, if \n- the opponent has set up the board so that it can win in the next move by dropping a disc in any of 2 or more columns, or \n- the only move that is available to the agent is one where, once played, the opponent can win in the next move.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> The agent can still lose the game, if \n- the opponent has set up the board so that it can win in the next move by dropping a disc in any of 2 or more columns, or \n- the only move that is available to the agent is one where, once played, the opponent can win in the next move.\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### 4) Create your own agent\n\nAmend the `my_agent()` function below to create your own agent.  Feel free to copy an agent that you created above.  \n\nNote that you'll have to include all of the necessary imports and helper functions.  For an example of how this would look with the first agent you created in the exercise, take a look at **[this notebook](https://www.kaggle.com/alexisbcook/create-a-connectx-agent)**.","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\ndef check_winning_move(obs, config, col, piece):\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    next_grid = drop_piece(grid, col, piece, config)\n    for row in range(config.rows):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[row, c:c+config.inarow]).count(piece) == config.inarow:\n                return True\n    for row in range(config.rows-(config.inarow-1)):\n        for c in range(config.columns):\n            if list(next_grid[row:row+config.inarow, c]).count(piece) == config.inarow:\n                return True\n    for row in range(config.rows-(config.inarow-1)):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[range(row, row+config.inarow), range(c, c+config.inarow)]).count(piece) == config.inarow:\n                return True\n    for row in range(config.inarow-1, config.rows):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[range(row, row-config.inarow, -1), range(c, c+config.inarow)]).count(piece) == config.inarow:\n                return True\n    return False\n\ndef my_agent(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    \n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    \n    opponent_mark = 1 if obs.mark == 2 else 2\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, opponent_mark):\n            return col\n    \n    return random.choice(valid_moves)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this code cell to get credit for creating an agent\nq_4.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:27:36.666905Z","iopub.execute_input":"2026-01-05T08:27:36.667202Z","iopub.status.idle":"2026-01-05T08:27:36.673316Z","shell.execute_reply.started":"2026-01-05T08:27:36.667183Z","shell.execute_reply":"2026-01-05T08:27:36.672362Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.2, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"4_CreateAgentEx1\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Thank you for creating an agent!","text/markdown":"<span style=\"color:#33cc33\">Thank you for creating an agent!</span>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"Run the next code cell to watch the agent play a game against the random agent.  You can re-run the code cell to play again!","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make\n\nenv = make(\"connectx\", debug=True)\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:27:45.262484Z","iopub.execute_input":"2026-01-05T08:27:45.263157Z","iopub.status.idle":"2026-01-05T08:28:18.119966Z","shell.execute_reply.started":"2026-01-05T08:27:45.263134Z","shell.execute_reply":"2026-01-05T08:28:18.118584Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import resource_stream, resource_exists\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n2026-01-05 08:28:04.659165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767601684.897219      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767601684.961917      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[kaggle_environments.envs.open_spiel.open_spiel] INFO: Successfully loaded OpenSpiel environments: 6.\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_chess\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_connect_four\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_gin_rummy\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_go\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_tic_tac_toe\n[kaggle_environments.envs.open_spiel.open_spiel] INFO:    open_spiel_universal_poker\n[kaggle_environments.envs.open_spiel.open_spiel] INFO: OpenSpiel games skipped: 0.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/1866143647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connectx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ipython\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'my_agent' is not defined"],"ename":"NameError","evalue":"name 'my_agent' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"### 5) Submit to the competition\n\nNow, it's time to make your first submission to the competition!  Run the next code cell to write your agent to a submission file.","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef drop_piece(grid, col, piece, config):\n    next_grid = grid.copy()\n    for row in range(config.rows-1, -1, -1):\n        if next_grid[row][col] == 0:\n            break\n    next_grid[row][col] = piece\n    return next_grid\n\ndef check_winning_move(obs, config, col, piece):\n    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n    next_grid = drop_piece(grid, col, piece, config)\n    for row in range(config.rows):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[row, c:c+config.inarow]).count(piece) == config.inarow:\n                return True\n    for row in range(config.rows-(config.inarow-1)):\n        for c in range(config.columns):\n            if list(next_grid[row:row+config.inarow, c]).count(piece) == config.inarow:\n                return True\n    for row in range(config.rows-(config.inarow-1)):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[range(row, row+config.inarow), range(c, c+config.inarow)]).count(piece) == config.inarow:\n                return True\n    for row in range(config.inarow-1, config.rows):\n        for c in range(config.columns-(config.inarow-1)):\n            if list(next_grid[range(row, row-config.inarow, -1), range(c, c+config.inarow)]).count(piece) == config.inarow:\n                return True\n    return False\n\ndef my_agent(obs, config):\n    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n    \n    for col in valid_moves:\n        if check_winning_move(obs, config, col, obs.mark):\n            return col\n    \n    opponent_mark = 1 if obs.mark == 2 else 2\n    for col in valid_moves:\n        if check_winning_move(obs, config, col, opponent_mark):\n            return col\n    \n    return random.choice(valid_moves)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:31:55.740939Z","iopub.execute_input":"2026-01-05T08:31:55.741211Z","iopub.status.idle":"2026-01-05T08:31:55.750340Z","shell.execute_reply.started":"2026-01-05T08:31:55.741191Z","shell.execute_reply":"2026-01-05T08:31:55.749375Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:31:59.283474Z","iopub.execute_input":"2026-01-05T08:31:59.283825Z","iopub.status.idle":"2026-01-05T08:31:59.290381Z","shell.execute_reply.started":"2026-01-05T08:31:59.283802Z","shell.execute_reply":"2026-01-05T08:31:59.289431Z"}},"outputs":[{"name":"stdout","text":"<function my_agent at 0x7fa9799e9440> written to submission.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"q_5.check()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:32:09.474506Z","iopub.execute_input":"2026-01-05T08:32:09.474804Z","iopub.status.idle":"2026-01-05T08:32:09.482431Z","shell.execute_reply.started":"2026-01-05T08:32:09.474788Z","shell.execute_reply":"2026-01-05T08:32:09.481545Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.2, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"5_SubmissionEx1\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Thank you for creating a submission file!","text/markdown":"<span style=\"color:#33cc33\">Thank you for creating a submission file!</span>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"Then, follow these steps:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Data** tab near the top of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\n\nGo to **\"My Submissions\"** to view your score and episodes being played.","metadata":{}},{"cell_type":"markdown","source":"# Keep going\n\nLearn how to **[use heuristics](https://www.kaggle.com/alexisbcook/one-step-lookahead)** to improve your agent.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion) to chat with other learners.*","metadata":{}}]}